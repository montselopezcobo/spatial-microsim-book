---
title: "Applying the methods of IPF and Combinatorial Optimisation"
author: "Morgane Dumont"
date: '`r Sys.Date()`'
output:
  ioslides_presentation: default
  beamer_presentation: default
  slidy_presentation: default
bibliography: ../bibliography.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE,eval=TRUE, warning=FALSE,message=FALSE}

ind <- read.csv("../data/CakeMap/ind.csv")
cons <- read.csv("../data/CakeMap/cons.csv")
# Load constraints separately - normally this would be first stage
con1 <- cons[1:12] # load the age/sex constraint
con2 <- cons[13:14] # load the car/no car constraint
con3 <- cons[15:24] # socio-economic class

# Rename the categories in "ind" to correspond to the one of cons
ind$Car <- sapply(ind$Car, FUN = switch, "Car", "NoCar")
ind$Sex <- sapply(ind$Sex, FUN = switch, "m", "f")
ind$NSSEC8 <- as.factor(ind$NSSEC8)
levels(ind$NSSEC8) <- colnames(con3)
ind$ageband4 <- 
  gsub(pattern = "-", replacement = "_", x = ind$ageband4)

# Initialise weights
weight_init_1zone <- table(ind)
init_cells <- rep(weight_init_1zone, each = nrow(cons))

# Define the names
names <- c(list(rownames(cons)),
           as.list(dimnames(weight_init_1zone)))

# Structure the data
weight_all <- array(init_cells, dim = 
                       c(nrow(cons), dim(weight_init_1zone)),
                     dimnames = names)

# Transform con1 into an 3D-array : con1_convert
names <- c(list(rownames(cons)),dimnames(weight_all)[c(4,6)])
con1_convert <- array(NA, dim=c(nrow(cons),2,6), dimnames = names)

for(zone in rownames(cons)){
  for (sex in dimnames(con1_convert)$Sex){
    for (age in dimnames(con1_convert)$ageband4){
      con1_convert[zone,sex,age] <- con1[zone,paste(sex,age,sep="")]
    }
  }
}

# Rescale con3 since it has some inconsistent constraints
con3_prop <- con3*rowSums(con2)/rowSums(con3)

# Load mipfp package
library(mipfp)

# Loop on the zones and make each time the mipfp
for (i in 1:nrow(cons)){
  target <- list(con1_convert[i,,],as.matrix(con2[i,]),as.matrix(con3_prop[i,]))
  descript <- list(c(3,5),2,4)
  res <- Ipfp(weight_init_1zone,descript,target)
  weight_all[i,,,,,] <- res$x.hat
}

# Results for zone 1
weight_init_1zone <- weight_all[1,,,,,]
```

## Plan for this part

    - Validation 
    
        - External
        - Internal
        - Empty cells
        
    - Population synthesis with integerisation 
    
        - Round
        - TRS
        
    - Introduction to Combinatorial Optimisation

## Validation 

There are two bigs groups of validations:

>  - External validation : we compare the results to external databases, not used to calibrate the simulation

>  - Internal validation : we compare the results to internal databaes, used to calibrate the simulation

## External validation

If spatial microsimulation was needed because of lack of data, no external database is available.

For testing the methods, we sometimes try to simulate a population we had at the microlevel.

Some new datasets could arrive later after the simulation and allow external validation.

## Internal validation

We first need to aggregate the result to obtain a table similar to *cons*.

```{r, echo=TRUE,eval=TRUE}
aggr <- apply(weight_all,c(1,6,4),sum)
aggr <- aggr[,,c(2,1)] # order of sex to fit cons
con2_sim = apply(weight_all,c(1,3),sum)
con3_sim = apply(weight_all,c(1,5),sum)
ind_agg <- cbind(as.data.frame(aggr),con2_sim,con3_sim)
```

##

Plot the simulated and target counts

```{r, echo=TRUE,eval=TRUE}
plot(as.matrix(ind_agg[1,]),as.matrix(cons[1,]))
```

##

Pearson's correlation

$$ r=\frac{s_{XY}}{S_X S_Y}=\frac{\frac{1}{n}\displaystyle\sum_{i=1}^n x_iy_i -\bar{x}\bar{y}}{\sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^n x_i^2-\bar{x}^2}\sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^n y_i^2-\bar{y}^2}}$$
This indicator measures the linear correlation between $X$ and $Y$.


In our case, the closest to one is the better.

##

CakeMap

```{r, echo=TRUE, eval=TRUE}
cor(as.vector(as.matrix(ind_agg)),as.vector(as.matrix(cons)))
```

We can also calculate the correlation per zone.

##

```{r, echo=TRUE}
# initialize the vector of correlations
CorVec <- rep (0, dim(cons)[1])

# calculate the correlation for each zone
for (i in 1:dim(cons)[1]){
  num_cons <- as.numeric(cons[i,])
  num_ind_agg <- as.numeric(ind_agg[i,])
  CorVec[i] <- cor (num_cons, num_ind_agg)
}
```

And to have an idea of the correlation for each zone, we can make a boxplot.

##
```{r, echo=TRUE}
boxplot(CorVec)
```

##
```{r, echo=TRUE}
# summary of the correlations per zone 
summary(CorVec)

# Identify the zone with the worst fit
which.min(CorVec)

# Top 3 worst values
head(order(CorVec), n = 3)
```
## 

Error between a simulation and the corresponding observation:

$$
e_{ij} = obs_{ij} - sim_{ij}
$$

where $obs$ and $sim$ are the observed and simulated values for each constraint category ($j$) and each area ($i$), respectively

Just making the sum of these errors doesn't allow to conclude on the total error, since some $e_{ij}$ will be negative numbers.

##

Total absolute error (TAE) is simply the sum of the absolute errors:


$$
TAE = \sum\limits_{ij} | e_{ij} |
$$

We can create a function that calculates *TAE* with *obs* and *sim* as input.

##

```{r, echo=TRUE}
tae <- function(observed, simulated){
  obs_vec <- as.matrix(observed)
  sim_vec <- as.matrix(simulated)
  sum(abs(obs_vec - sim_vec))
}
```

For Cakemap

```{r, echo=TRUE}
tae(cons, ind_agg)
``` 

## 

Relative Error (RE)

$$
RE = \frac{TAE}{(total\_pop * n\_var)}
$$

We can also calculate this RE per variable and par zone

$$
RE(var_i, zone_j) = \frac{TAE_{ij}}{total\_pop}
$$

##

RE :
```{r, echo=TRUE}
tae(cons, ind_agg) / sum(cons) 
```

```{r}
# Initialize the vectors
TAEVec <- rep(0, nrow(cons))
REVec <- rep(0, nrow(cons))

# calculate the correlation for each zone
for (i in 1:nrow(cons)){
  TAEVec[i] <- tae (cons[i,], ind_agg[i,])
  REVec[i] <- TAEVec[i] / sum(cons[i,])
}
```

You can also make an analysis of these vectors.

## 
Root mean squared error (RMSE)

$$
RMSE = \sqrt{\frac{1}{n} \sum_i^n e^2_i}
$$


RMSE is an interesting measure of the error, since TAE and RE would be the same if the errors are $(1,1,1,1)$ or
$(0,0,0,4)$. 

##
Normalised RMSE:

$$
NRMSE = \frac{RMSE}{max(obs) - min(obs)} 
$$

## 
Chi-squared

$$
\chi^2= \sum_{i=1}^{n\_line}\sum_{j=1}^{n\_column}\frac{(sim_{ij} - obs_{ij})^2}{obs_{ij}}
$$

$p-value$ = probability to observe the simulation or a worse if we consider that $obs$ and $sim$ are independent.

## Empty cells

What will happen if the sample has this corresponding cross-table?

|sex - age| 0-49 yrs| 50 + yrs| Total|
|:-------------|-----:|-----:|-----:|
|m             |     1|     2|     3|
|f             |     1|     0|     1|
|Total         |     2|     2|      |
        
        
##

We need to check if zeros are only in categories that are 'impossible'.


## Population synthesis with integerisation 
    
The final aim is not to have a weight table, but an individual dataset. So we need to transform the weights into integer (since half a person has no sense).
    
If you try to simply round the weights, what happen?

> - round((0.67;0.7;2.63))=(1;1;3), so we have finally 5 instead of 4 persons.
> - round((0.333;1.333;0.334))=(0,1,0), so we have finally 1 instead of 2 persons.
    
##
A proposed method to fix this is the TRS [@lovelace_truncate_2013]:

> 1. Truncate : that only the integer part
> 2. Replicate : for each cell (combination of category), create the number of individuals indicated by the truncate process.
> 3. Sample : consider the decimals of the weight as probabilities and make a random draw out of this distribution until having the good number of individuals.

Try to code this method and save the individual level result in a file.
        
##

```{r, echo=TRUE,eval=TRUE}
flat_weights <- as.data.frame.table(weight_init_1zone, 
                                    responseName = "COUNT") 
```


```{r, echo=TRUE,eval=TRUE}
truncated <- flat_weights
truncated$COUNT <- floor(truncated$COUNT)
p <- flat_weights$COUNT - truncated$COUNT
n_missing <- sum(p)
index <- sample(1:nrow(truncated),size = n_missing, replace=F)
flat_weights$COUNT[index] <- flat_weights$COUNT[index]+1
```


## Alternative approach: optimisation

Classic optimisation problem:

$$\left\{
\begin{array}{l}
  min \hspace{0.2cm}f_0(x_1,x_2,..,x_n) \\
  \\
  s.c. \hspace{0.2cm} f_i(x) \geq par_i,\  i = 1, ..., m
\end{array}
\right.$$

## 

In spatial microsimulation:


$$ \left\{ \begin{array}{l}
  min \hspace{0.2cm} f(w_1,..,w_m) = DIST(ind_agg, cons) \\
  \\
  s.c. \hspace{0.2cm} w_i \geq 0,\  i = 1, ..., m
\end{array} \right. $$

## Combinatorial Optimisation

An optimisation problem where the domain is a set of different combination. 

In our case, we would like to define a combination of the individuals in the sample that minimises the distance between
the final aggregated count and the constraints.

When only few combinations are possible, we can simply to each one and measure the distance to choose the minimum.

When they are a wide range of possible combinations, we have heuristic methods:

- Simulated annealing
- Genetic algorithms
- Tabu search

##

As an example, we will explain how a genetic algorithms works.

> - initialisation : we have a population of possible candidates (combinations of the individuals in the sample)
> - evaluation: we calculate the distance between agregated counts and constraints for each candidate
> - selection: we keep only a part of the population and the probability of each candidates to be selected is his evaluation
> - cross over : two candidates will be mixed to create a children candidate
> - mutation : some candidates will mute
> - until a stopping criterion

## 
Advantages : 
These kind of methods doesn't need an integerisation step since it directly gives the number of times to replicate each individual.

Some functions to optimise in R: *genoud*, *optim*, *genSA*,...

## References